{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/janestreet/data.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook's approach is using Neural Network.\n",
    "\n",
    "* Ver 2: NN\n",
    "* Ver 3: NN + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings (\"ignore\")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import gc  \n",
    "\n",
    "import tensorflow as tf\n",
    "SEED = 1111\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>feature_79</th>\n",
       "      <th>feature_80</th>\n",
       "      <th>feature_81</th>\n",
       "      <th>feature_82</th>\n",
       "      <th>feature_83</th>\n",
       "      <th>feature_84</th>\n",
       "      <th>feature_85</th>\n",
       "      <th>feature_86</th>\n",
       "      <th>feature_87</th>\n",
       "      <th>feature_88</th>\n",
       "      <th>feature_89</th>\n",
       "      <th>feature_90</th>\n",
       "      <th>feature_91</th>\n",
       "      <th>feature_92</th>\n",
       "      <th>feature_93</th>\n",
       "      <th>feature_94</th>\n",
       "      <th>feature_95</th>\n",
       "      <th>feature_96</th>\n",
       "      <th>feature_97</th>\n",
       "      <th>feature_98</th>\n",
       "      <th>feature_99</th>\n",
       "      <th>feature_100</th>\n",
       "      <th>feature_101</th>\n",
       "      <th>feature_102</th>\n",
       "      <th>feature_103</th>\n",
       "      <th>feature_104</th>\n",
       "      <th>feature_105</th>\n",
       "      <th>feature_106</th>\n",
       "      <th>feature_107</th>\n",
       "      <th>feature_108</th>\n",
       "      <th>feature_109</th>\n",
       "      <th>feature_110</th>\n",
       "      <th>feature_111</th>\n",
       "      <th>feature_112</th>\n",
       "      <th>feature_113</th>\n",
       "      <th>feature_114</th>\n",
       "      <th>feature_115</th>\n",
       "      <th>feature_116</th>\n",
       "      <th>feature_117</th>\n",
       "      <th>feature_118</th>\n",
       "      <th>feature_119</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>ts_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86</td>\n",
       "      <td>0.859516</td>\n",
       "      <td>-0.003656</td>\n",
       "      <td>-0.005449</td>\n",
       "      <td>-0.017403</td>\n",
       "      <td>-0.028896</td>\n",
       "      <td>-0.021435</td>\n",
       "      <td>1</td>\n",
       "      <td>3.151305</td>\n",
       "      <td>5.467693</td>\n",
       "      <td>-0.164505</td>\n",
       "      <td>-0.189219</td>\n",
       "      <td>0.663966</td>\n",
       "      <td>0.988896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.184804</td>\n",
       "      <td>3.278742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.471544</td>\n",
       "      <td>2.391430</td>\n",
       "      <td>1.640887</td>\n",
       "      <td>3.938759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.361346</td>\n",
       "      <td>4.711640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.958027</td>\n",
       "      <td>4.069699</td>\n",
       "      <td>2.535238</td>\n",
       "      <td>4.813858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.194392</td>\n",
       "      <td>-0.336857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>-0.131142</td>\n",
       "      <td>-0.197839</td>\n",
       "      <td>-0.288336</td>\n",
       "      <td>0.341815</td>\n",
       "      <td>0.599994</td>\n",
       "      <td>-0.202268</td>\n",
       "      <td>-0.471068</td>\n",
       "      <td>-0.405654</td>\n",
       "      <td>0.052440</td>\n",
       "      <td>-1.349263</td>\n",
       "      <td>-0.633717</td>\n",
       "      <td>-0.796947</td>\n",
       "      <td>-0.381209</td>\n",
       "      <td>-0.542896</td>\n",
       "      <td>-0.166690</td>\n",
       "      <td>0.222588</td>\n",
       "      <td>-0.554461</td>\n",
       "      <td>-0.187021</td>\n",
       "      <td>-0.551972</td>\n",
       "      <td>2.589171</td>\n",
       "      <td>4.141244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.414845</td>\n",
       "      <td>-1.150279</td>\n",
       "      <td>-0.908903</td>\n",
       "      <td>-1.341168</td>\n",
       "      <td>-1.325681</td>\n",
       "      <td>-1.110222</td>\n",
       "      <td>-1.656347</td>\n",
       "      <td>-1.810199</td>\n",
       "      <td>-4.434427</td>\n",
       "      <td>-1.661965</td>\n",
       "      <td>-1.101438</td>\n",
       "      <td>-2.136155</td>\n",
       "      <td>-2.048593</td>\n",
       "      <td>2.649794</td>\n",
       "      <td>3.595955</td>\n",
       "      <td>4.039769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.254960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.763423</td>\n",
       "      <td>-1.204365</td>\n",
       "      <td>0.649855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.374423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.335769</td>\n",
       "      <td>-0.345847</td>\n",
       "      <td>0.386626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.238212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.321156</td>\n",
       "      <td>-1.641860</td>\n",
       "      <td>-2.060506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.515613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.746285</td>\n",
       "      <td>-1.086886</td>\n",
       "      <td>-3.388070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.607253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.199793</td>\n",
       "      <td>0.969663</td>\n",
       "      <td>-2.434601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.227364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.633981</td>\n",
       "      <td>-0.122468</td>\n",
       "      <td>-4.349793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.322244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.458309</td>\n",
       "      <td>-0.032740</td>\n",
       "      <td>-3.018269</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.219454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.608786</td>\n",
       "      <td>-1.611309</td>\n",
       "      <td>-2.724954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.433699</td>\n",
       "      <td>4.282284</td>\n",
       "      <td>1.621115</td>\n",
       "      <td>4.331030</td>\n",
       "      <td>2.553220</td>\n",
       "      <td>3.799011</td>\n",
       "      <td>2.642943</td>\n",
       "      <td>3.998054</td>\n",
       "      <td>527894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009107</td>\n",
       "      <td>-0.013542</td>\n",
       "      <td>-0.022222</td>\n",
       "      <td>-0.032522</td>\n",
       "      <td>-0.026394</td>\n",
       "      <td>1</td>\n",
       "      <td>2.249176</td>\n",
       "      <td>2.618401</td>\n",
       "      <td>-0.304355</td>\n",
       "      <td>-0.276975</td>\n",
       "      <td>-0.035921</td>\n",
       "      <td>-0.036215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.354857</td>\n",
       "      <td>3.040463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.365050</td>\n",
       "      <td>2.376956</td>\n",
       "      <td>2.337125</td>\n",
       "      <td>3.438553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.041641</td>\n",
       "      <td>4.165903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.889146</td>\n",
       "      <td>4.174374</td>\n",
       "      <td>3.234317</td>\n",
       "      <td>4.276899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.644735</td>\n",
       "      <td>-2.479335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.321317</td>\n",
       "      <td>-1.491122</td>\n",
       "      <td>-2.478752</td>\n",
       "      <td>-2.496164</td>\n",
       "      <td>0.396227</td>\n",
       "      <td>0.435508</td>\n",
       "      <td>-0.248213</td>\n",
       "      <td>-0.439213</td>\n",
       "      <td>-0.993568</td>\n",
       "      <td>3.075146</td>\n",
       "      <td>0.017041</td>\n",
       "      <td>-1.907786</td>\n",
       "      <td>-1.643755</td>\n",
       "      <td>-0.873190</td>\n",
       "      <td>-0.730260</td>\n",
       "      <td>-0.627203</td>\n",
       "      <td>-0.498971</td>\n",
       "      <td>-2.203055</td>\n",
       "      <td>-1.725478</td>\n",
       "      <td>-2.086345</td>\n",
       "      <td>2.184227</td>\n",
       "      <td>-0.152785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.687010</td>\n",
       "      <td>-1.775500</td>\n",
       "      <td>-1.508644</td>\n",
       "      <td>-1.546806</td>\n",
       "      <td>-1.248566</td>\n",
       "      <td>-1.045781</td>\n",
       "      <td>-1.892714</td>\n",
       "      <td>-2.131570</td>\n",
       "      <td>-4.407797</td>\n",
       "      <td>-1.720470</td>\n",
       "      <td>-1.133543</td>\n",
       "      <td>-2.126616</td>\n",
       "      <td>-2.039426</td>\n",
       "      <td>3.591019</td>\n",
       "      <td>1.079669</td>\n",
       "      <td>3.550142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.871530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.154754</td>\n",
       "      <td>-1.803521</td>\n",
       "      <td>-0.008046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.198914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.167805</td>\n",
       "      <td>-0.182515</td>\n",
       "      <td>0.164888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.238212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.321156</td>\n",
       "      <td>-1.641860</td>\n",
       "      <td>-2.579694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.515613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.746285</td>\n",
       "      <td>-1.086886</td>\n",
       "      <td>-4.781603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.557578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.665543</td>\n",
       "      <td>1.704761</td>\n",
       "      <td>-1.965635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.079505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.857492</td>\n",
       "      <td>-0.512759</td>\n",
       "      <td>-4.546557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.275872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.054892</td>\n",
       "      <td>0.872509</td>\n",
       "      <td>-3.120828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.881751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.280218</td>\n",
       "      <td>-2.261787</td>\n",
       "      <td>-3.617442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.053416</td>\n",
       "      <td>-0.493276</td>\n",
       "      <td>1.661974</td>\n",
       "      <td>-1.082122</td>\n",
       "      <td>2.427706</td>\n",
       "      <td>-0.756115</td>\n",
       "      <td>2.210572</td>\n",
       "      <td>-0.639075</td>\n",
       "      <td>527895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86</td>\n",
       "      <td>0.590949</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>-0.000376</td>\n",
       "      <td>-0.004051</td>\n",
       "      <td>-0.007995</td>\n",
       "      <td>-0.004743</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.365888</td>\n",
       "      <td>0.824004</td>\n",
       "      <td>-0.293208</td>\n",
       "      <td>-0.416391</td>\n",
       "      <td>-0.599185</td>\n",
       "      <td>-0.997330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.869330</td>\n",
       "      <td>0.174646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.376733</td>\n",
       "      <td>-2.602154</td>\n",
       "      <td>-0.580833</td>\n",
       "      <td>0.145479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.440224</td>\n",
       "      <td>-0.943834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.842764</td>\n",
       "      <td>-3.478558</td>\n",
       "      <td>-0.506549</td>\n",
       "      <td>-1.058953</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.539967</td>\n",
       "      <td>1.481719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.533328</td>\n",
       "      <td>1.164644</td>\n",
       "      <td>0.958275</td>\n",
       "      <td>1.936930</td>\n",
       "      <td>-0.550514</td>\n",
       "      <td>-0.926700</td>\n",
       "      <td>0.055286</td>\n",
       "      <td>0.153123</td>\n",
       "      <td>1.277755</td>\n",
       "      <td>-2.542437</td>\n",
       "      <td>-2.073995</td>\n",
       "      <td>-1.939970</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>-1.060566</td>\n",
       "      <td>-2.328535</td>\n",
       "      <td>-1.710825</td>\n",
       "      <td>-1.137115</td>\n",
       "      <td>-0.845435</td>\n",
       "      <td>-0.524127</td>\n",
       "      <td>-0.569021</td>\n",
       "      <td>-0.570763</td>\n",
       "      <td>3.906561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.536226</td>\n",
       "      <td>-1.788258</td>\n",
       "      <td>-1.481269</td>\n",
       "      <td>-1.096765</td>\n",
       "      <td>-1.349678</td>\n",
       "      <td>-1.121166</td>\n",
       "      <td>-1.152951</td>\n",
       "      <td>-1.245204</td>\n",
       "      <td>-4.244271</td>\n",
       "      <td>-1.639473</td>\n",
       "      <td>-1.091728</td>\n",
       "      <td>-2.539531</td>\n",
       "      <td>-2.422194</td>\n",
       "      <td>-0.437467</td>\n",
       "      <td>3.325025</td>\n",
       "      <td>0.603514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.191418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.305311</td>\n",
       "      <td>0.127925</td>\n",
       "      <td>-0.016398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.272357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.107799</td>\n",
       "      <td>0.167305</td>\n",
       "      <td>-0.412543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.138030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.607754</td>\n",
       "      <td>-0.449674</td>\n",
       "      <td>-2.954607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.092611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.372403</td>\n",
       "      <td>-0.066319</td>\n",
       "      <td>-2.740989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.338859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.018706</td>\n",
       "      <td>-0.522890</td>\n",
       "      <td>-2.132602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.940190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.034410</td>\n",
       "      <td>-0.590374</td>\n",
       "      <td>-2.151733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.471300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.385969</td>\n",
       "      <td>-2.290683</td>\n",
       "      <td>-3.531129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.673329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.017174</td>\n",
       "      <td>-1.059342</td>\n",
       "      <td>-1.723941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.702873</td>\n",
       "      <td>4.038753</td>\n",
       "      <td>-0.789767</td>\n",
       "      <td>4.133183</td>\n",
       "      <td>-1.207878</td>\n",
       "      <td>3.402796</td>\n",
       "      <td>-0.928290</td>\n",
       "      <td>3.511141</td>\n",
       "      <td>527896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86</td>\n",
       "      <td>0.172997</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>-0.002375</td>\n",
       "      <td>-0.003064</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>1</td>\n",
       "      <td>1.514607</td>\n",
       "      <td>0.596214</td>\n",
       "      <td>0.324062</td>\n",
       "      <td>0.154730</td>\n",
       "      <td>0.845069</td>\n",
       "      <td>0.521491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.310387</td>\n",
       "      <td>-0.379196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.866451</td>\n",
       "      <td>0.148476</td>\n",
       "      <td>0.197457</td>\n",
       "      <td>-0.516572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.025831</td>\n",
       "      <td>0.704435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.691567</td>\n",
       "      <td>1.379021</td>\n",
       "      <td>1.111965</td>\n",
       "      <td>0.682265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.635982</td>\n",
       "      <td>-0.525029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.458078</td>\n",
       "      <td>-0.246643</td>\n",
       "      <td>-0.916675</td>\n",
       "      <td>-0.482240</td>\n",
       "      <td>0.590027</td>\n",
       "      <td>0.381223</td>\n",
       "      <td>-0.033720</td>\n",
       "      <td>-0.019842</td>\n",
       "      <td>-0.368249</td>\n",
       "      <td>1.269972</td>\n",
       "      <td>1.796582</td>\n",
       "      <td>-1.092631</td>\n",
       "      <td>-0.284514</td>\n",
       "      <td>-0.309811</td>\n",
       "      <td>-0.448508</td>\n",
       "      <td>-0.685631</td>\n",
       "      <td>-0.774103</td>\n",
       "      <td>-0.838765</td>\n",
       "      <td>-1.379772</td>\n",
       "      <td>0.770823</td>\n",
       "      <td>2.462665</td>\n",
       "      <td>1.660164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.900839</td>\n",
       "      <td>-1.338204</td>\n",
       "      <td>-0.951015</td>\n",
       "      <td>-0.624533</td>\n",
       "      <td>-1.730530</td>\n",
       "      <td>-1.460900</td>\n",
       "      <td>-1.126970</td>\n",
       "      <td>-1.216374</td>\n",
       "      <td>-4.175624</td>\n",
       "      <td>-1.571539</td>\n",
       "      <td>-1.050961</td>\n",
       "      <td>-3.270302</td>\n",
       "      <td>-3.084209</td>\n",
       "      <td>0.740927</td>\n",
       "      <td>-0.680706</td>\n",
       "      <td>-0.093278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.172843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.113724</td>\n",
       "      <td>-0.148526</td>\n",
       "      <td>0.106049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.109105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.135978</td>\n",
       "      <td>-0.098433</td>\n",
       "      <td>0.069875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.238212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.321156</td>\n",
       "      <td>-1.641860</td>\n",
       "      <td>-3.581284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.515613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.746285</td>\n",
       "      <td>-1.086886</td>\n",
       "      <td>-4.438488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.140773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.762597</td>\n",
       "      <td>-0.409249</td>\n",
       "      <td>-2.698973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.252248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.906413</td>\n",
       "      <td>-0.748366</td>\n",
       "      <td>-3.765935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.338233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.568599</td>\n",
       "      <td>-2.851826</td>\n",
       "      <td>-4.757620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.294113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.416992</td>\n",
       "      <td>-2.645002</td>\n",
       "      <td>-2.973197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.304354</td>\n",
       "      <td>1.530169</td>\n",
       "      <td>3.596848</td>\n",
       "      <td>4.613493</td>\n",
       "      <td>4.516110</td>\n",
       "      <td>3.341374</td>\n",
       "      <td>2.635798</td>\n",
       "      <td>1.535235</td>\n",
       "      <td>527897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.001587</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.158576</td>\n",
       "      <td>-0.146579</td>\n",
       "      <td>-0.035525</td>\n",
       "      <td>-0.008082</td>\n",
       "      <td>-0.152708</td>\n",
       "      <td>-0.251737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.420522</td>\n",
       "      <td>0.883579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.256021</td>\n",
       "      <td>-0.943030</td>\n",
       "      <td>-0.052071</td>\n",
       "      <td>1.562831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.351357</td>\n",
       "      <td>-0.788583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.282124</td>\n",
       "      <td>-0.737988</td>\n",
       "      <td>-0.388627</td>\n",
       "      <td>-0.867531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.793271</td>\n",
       "      <td>2.338712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.828729</td>\n",
       "      <td>4.012945</td>\n",
       "      <td>1.534470</td>\n",
       "      <td>3.294522</td>\n",
       "      <td>-0.193584</td>\n",
       "      <td>-0.327048</td>\n",
       "      <td>0.195077</td>\n",
       "      <td>0.648519</td>\n",
       "      <td>3.548811</td>\n",
       "      <td>-2.674746</td>\n",
       "      <td>-2.498546</td>\n",
       "      <td>-0.419519</td>\n",
       "      <td>0.522173</td>\n",
       "      <td>-1.923837</td>\n",
       "      <td>-3.759887</td>\n",
       "      <td>-2.987109</td>\n",
       "      <td>-1.977894</td>\n",
       "      <td>-2.407830</td>\n",
       "      <td>-1.240331</td>\n",
       "      <td>-0.455841</td>\n",
       "      <td>-1.435899</td>\n",
       "      <td>1.509234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.974874</td>\n",
       "      <td>-0.436446</td>\n",
       "      <td>0.390016</td>\n",
       "      <td>1.458579</td>\n",
       "      <td>-1.394944</td>\n",
       "      <td>-1.166572</td>\n",
       "      <td>-1.160680</td>\n",
       "      <td>-1.253341</td>\n",
       "      <td>-4.121538</td>\n",
       "      <td>-1.608494</td>\n",
       "      <td>-1.071674</td>\n",
       "      <td>-2.468594</td>\n",
       "      <td>-2.358795</td>\n",
       "      <td>-1.591525</td>\n",
       "      <td>0.332764</td>\n",
       "      <td>-0.607468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.160988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.636603</td>\n",
       "      <td>-0.245092</td>\n",
       "      <td>-0.002111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.198455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.009445</td>\n",
       "      <td>-0.445887</td>\n",
       "      <td>-0.343653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.055298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.141359</td>\n",
       "      <td>-0.145292</td>\n",
       "      <td>-2.477622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.245049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.245978</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>-2.195447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.353913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.585657</td>\n",
       "      <td>0.138084</td>\n",
       "      <td>-1.916957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.262477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.252213</td>\n",
       "      <td>1.328353</td>\n",
       "      <td>-1.874398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.951431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.212780</td>\n",
       "      <td>-0.854314</td>\n",
       "      <td>-3.008724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.004068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.417244</td>\n",
       "      <td>0.806850</td>\n",
       "      <td>-1.460248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.364269</td>\n",
       "      <td>2.006926</td>\n",
       "      <td>-1.237922</td>\n",
       "      <td>2.124396</td>\n",
       "      <td>-2.005723</td>\n",
       "      <td>1.716914</td>\n",
       "      <td>-1.646484</td>\n",
       "      <td>1.795211</td>\n",
       "      <td>527898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date    weight    resp_1    resp_2    resp_3    resp_4      resp  \\\n",
       "0    86  0.859516 -0.003656 -0.005449 -0.017403 -0.028896 -0.021435   \n",
       "1    86  0.000000 -0.009107 -0.013542 -0.022222 -0.032522 -0.026394   \n",
       "2    86  0.590949  0.000347 -0.000376 -0.004051 -0.007995 -0.004743   \n",
       "3    86  0.172997  0.000168  0.000333 -0.002375 -0.003064  0.001527   \n",
       "4    86  0.000000  0.000503  0.000589 -0.001587 -0.002665 -0.000139   \n",
       "\n",
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0          1   3.151305   5.467693  -0.164505  -0.189219   0.663966   \n",
       "1          1   2.249176   2.618401  -0.304355  -0.276975  -0.035921   \n",
       "2         -1  -0.365888   0.824004  -0.293208  -0.416391  -0.599185   \n",
       "3          1   1.514607   0.596214   0.324062   0.154730   0.845069   \n",
       "4         -1  -1.158576  -0.146579  -0.035525  -0.008082  -0.152708   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
       "0   0.988896        NaN        NaN   2.184804    3.278742         NaN   \n",
       "1  -0.036215        NaN        NaN   3.354857    3.040463         NaN   \n",
       "2  -0.997330        NaN        NaN  -0.869330    0.174646         NaN   \n",
       "3   0.521491        NaN        NaN   0.310387   -0.379196         NaN   \n",
       "4  -0.251737        NaN        NaN  -0.420522    0.883579         NaN   \n",
       "\n",
       "   feature_12  feature_13  feature_14  feature_15  feature_16  feature_17  \\\n",
       "0         NaN    1.471544    2.391430    1.640887    3.938759         NaN   \n",
       "1         NaN    2.365050    2.376956    2.337125    3.438553         NaN   \n",
       "2         NaN   -2.376733   -2.602154   -0.580833    0.145479         NaN   \n",
       "3         NaN    0.866451    0.148476    0.197457   -0.516572         NaN   \n",
       "4         NaN   -1.256021   -0.943030   -0.052071    1.562831         NaN   \n",
       "\n",
       "   feature_18  feature_19  feature_20  feature_21  feature_22  feature_23  \\\n",
       "0         NaN    2.361346    4.711640         NaN         NaN    1.958027   \n",
       "1         NaN    3.041641    4.165903         NaN         NaN    2.889146   \n",
       "2         NaN   -0.440224   -0.943834         NaN         NaN   -1.842764   \n",
       "3         NaN    1.025831    0.704435         NaN         NaN    1.691567   \n",
       "4         NaN   -0.351357   -0.788583         NaN         NaN   -0.282124   \n",
       "\n",
       "   feature_24  feature_25  feature_26  feature_27  feature_28  feature_29  \\\n",
       "0    4.069699    2.535238    4.813858         NaN         NaN   -0.194392   \n",
       "1    4.174374    3.234317    4.276899         NaN         NaN   -1.644735   \n",
       "2   -3.478558   -0.506549   -1.058953         NaN         NaN    0.539967   \n",
       "3    1.379021    1.111965    0.682265         NaN         NaN   -0.635982   \n",
       "4   -0.737988   -0.388627   -0.867531         NaN         NaN    0.793271   \n",
       "\n",
       "   feature_30  feature_31  feature_32  feature_33  feature_34  feature_35  \\\n",
       "0   -0.336857         NaN         NaN   -0.073242   -0.131142   -0.197839   \n",
       "1   -2.479335         NaN         NaN   -1.321317   -1.491122   -2.478752   \n",
       "2    1.481719         NaN         NaN    0.533328    1.164644    0.958275   \n",
       "3   -0.525029         NaN         NaN   -0.458078   -0.246643   -0.916675   \n",
       "4    2.338712         NaN         NaN    1.828729    4.012945    1.534470   \n",
       "\n",
       "   feature_36  feature_37  feature_38  feature_39  feature_40  feature_41  \\\n",
       "0   -0.288336    0.341815    0.599994   -0.202268   -0.471068   -0.405654   \n",
       "1   -2.496164    0.396227    0.435508   -0.248213   -0.439213   -0.993568   \n",
       "2    1.936930   -0.550514   -0.926700    0.055286    0.153123    1.277755   \n",
       "3   -0.482240    0.590027    0.381223   -0.033720   -0.019842   -0.368249   \n",
       "4    3.294522   -0.193584   -0.327048    0.195077    0.648519    3.548811   \n",
       "\n",
       "   feature_42  feature_43  feature_44  feature_45  feature_46  feature_47  \\\n",
       "0    0.052440   -1.349263   -0.633717   -0.796947   -0.381209   -0.542896   \n",
       "1    3.075146    0.017041   -1.907786   -1.643755   -0.873190   -0.730260   \n",
       "2   -2.542437   -2.073995   -1.939970   -0.023424   -1.060566   -2.328535   \n",
       "3    1.269972    1.796582   -1.092631   -0.284514   -0.309811   -0.448508   \n",
       "4   -2.674746   -2.498546   -0.419519    0.522173   -1.923837   -3.759887   \n",
       "\n",
       "   feature_48  feature_49  feature_50  feature_51  feature_52  feature_53  \\\n",
       "0   -0.166690    0.222588   -0.554461   -0.187021   -0.551972    2.589171   \n",
       "1   -0.627203   -0.498971   -2.203055   -1.725478   -2.086345    2.184227   \n",
       "2   -1.710825   -1.137115   -0.845435   -0.524127   -0.569021   -0.570763   \n",
       "3   -0.685631   -0.774103   -0.838765   -1.379772    0.770823    2.462665   \n",
       "4   -2.987109   -1.977894   -2.407830   -1.240331   -0.455841   -1.435899   \n",
       "\n",
       "   feature_54  feature_55  feature_56  feature_57  feature_58  feature_59  \\\n",
       "0    4.141244         NaN   -0.414845   -1.150279   -0.908903   -1.341168   \n",
       "1   -0.152785         NaN   -0.687010   -1.775500   -1.508644   -1.546806   \n",
       "2    3.906561         NaN   -0.536226   -1.788258   -1.481269   -1.096765   \n",
       "3    1.660164         NaN   -0.900839   -1.338204   -0.951015   -0.624533   \n",
       "4    1.509234         NaN    0.974874   -0.436446    0.390016    1.458579   \n",
       "\n",
       "   feature_60  feature_61  feature_62  feature_63  feature_64  feature_65  \\\n",
       "0   -1.325681   -1.110222   -1.656347   -1.810199   -4.434427   -1.661965   \n",
       "1   -1.248566   -1.045781   -1.892714   -2.131570   -4.407797   -1.720470   \n",
       "2   -1.349678   -1.121166   -1.152951   -1.245204   -4.244271   -1.639473   \n",
       "3   -1.730530   -1.460900   -1.126970   -1.216374   -4.175624   -1.571539   \n",
       "4   -1.394944   -1.166572   -1.160680   -1.253341   -4.121538   -1.608494   \n",
       "\n",
       "   feature_66  feature_67  feature_68  feature_69  feature_70  feature_71  \\\n",
       "0   -1.101438   -2.136155   -2.048593    2.649794    3.595955    4.039769   \n",
       "1   -1.133543   -2.126616   -2.039426    3.591019    1.079669    3.550142   \n",
       "2   -1.091728   -2.539531   -2.422194   -0.437467    3.325025    0.603514   \n",
       "3   -1.050961   -3.270302   -3.084209    0.740927   -0.680706   -0.093278   \n",
       "4   -1.071674   -2.468594   -2.358795   -1.591525    0.332764   -0.607468   \n",
       "\n",
       "   feature_72  feature_73  feature_74  feature_75  feature_76  feature_77  \\\n",
       "0         NaN   -1.254960         NaN   -0.763423   -1.204365    0.649855   \n",
       "1         NaN   -1.871530         NaN   -1.154754   -1.803521   -0.008046   \n",
       "2         NaN    0.191418         NaN   -0.305311    0.127925   -0.016398   \n",
       "3         NaN   -0.172843         NaN   -0.113724   -0.148526    0.106049   \n",
       "4         NaN    0.160988         NaN   -0.636603   -0.245092   -0.002111   \n",
       "\n",
       "   feature_78  feature_79  feature_80  feature_81  feature_82  feature_83  \\\n",
       "0         NaN   -0.374423         NaN   -0.335769   -0.345847    0.386626   \n",
       "1         NaN   -0.198914         NaN   -0.167805   -0.182515    0.164888   \n",
       "2         NaN    0.272357         NaN   -1.107799    0.167305   -0.412543   \n",
       "3         NaN   -0.109105         NaN   -0.135978   -0.098433    0.069875   \n",
       "4         NaN    0.198455         NaN   -2.009445   -0.445887   -0.343653   \n",
       "\n",
       "   feature_84  feature_85  feature_86  feature_87  feature_88  feature_89  \\\n",
       "0         NaN   -1.238212         NaN   -2.321156   -1.641860   -2.060506   \n",
       "1         NaN   -1.238212         NaN   -2.321156   -1.641860   -2.579694   \n",
       "2         NaN   -0.138030         NaN   -0.607754   -0.449674   -2.954607   \n",
       "3         NaN   -1.238212         NaN   -2.321156   -1.641860   -3.581284   \n",
       "4         NaN   -0.055298         NaN    0.141359   -0.145292   -2.477622   \n",
       "\n",
       "   feature_90  feature_91  feature_92  feature_93  feature_94  feature_95  \\\n",
       "0         NaN   -1.515613         NaN   -1.746285   -1.086886   -3.388070   \n",
       "1         NaN   -1.515613         NaN   -1.746285   -1.086886   -4.781603   \n",
       "2         NaN    0.092611         NaN    0.372403   -0.066319   -2.740989   \n",
       "3         NaN   -1.515613         NaN   -1.746285   -1.086886   -4.438488   \n",
       "4         NaN    0.245049         NaN    1.245978    0.345200   -2.195447   \n",
       "\n",
       "   feature_96  feature_97  feature_98  feature_99  feature_100  feature_101  \\\n",
       "0         NaN    1.607253         NaN    0.199793     0.969663    -2.434601   \n",
       "1         NaN    2.557578         NaN    0.665543     1.704761    -1.965635   \n",
       "2         NaN   -1.338859         NaN   -0.018706    -0.522890    -2.132602   \n",
       "3         NaN   -0.140773         NaN   -0.762597    -0.409249    -2.698973   \n",
       "4         NaN   -0.353913         NaN    0.585657     0.138084    -1.916957   \n",
       "\n",
       "   feature_102  feature_103  feature_104  feature_105  feature_106  \\\n",
       "0          NaN     0.227364          NaN    -0.633981    -0.122468   \n",
       "1          NaN    -0.079505          NaN    -0.857492    -0.512759   \n",
       "2          NaN    -0.940190          NaN     1.034410    -0.590374   \n",
       "3          NaN    -0.252248          NaN    -0.906413    -0.748366   \n",
       "4          NaN    -0.262477          NaN     2.252213     1.328353   \n",
       "\n",
       "   feature_107  feature_108  feature_109  feature_110  feature_111  \\\n",
       "0    -4.349793          NaN     0.322244          NaN    -0.458309   \n",
       "1    -4.546557          NaN     1.275872          NaN    -0.054892   \n",
       "2    -2.151733          NaN    -2.471300          NaN    -0.385969   \n",
       "3    -3.765935          NaN    -2.338233          NaN    -1.568599   \n",
       "4    -1.874398          NaN    -1.951431          NaN     0.212780   \n",
       "\n",
       "   feature_112  feature_113  feature_114  feature_115  feature_116  \\\n",
       "0    -0.032740    -3.018269          NaN    -1.219454          NaN   \n",
       "1     0.872509    -3.120828          NaN    -1.881751          NaN   \n",
       "2    -2.290683    -3.531129          NaN    -1.673329          NaN   \n",
       "3    -2.851826    -4.757620          NaN    -2.294113          NaN   \n",
       "4    -0.854314    -3.008724          NaN    -1.004068          NaN   \n",
       "\n",
       "   feature_117  feature_118  feature_119  feature_120  feature_121  \\\n",
       "0    -2.608786    -1.611309    -2.724954          NaN          NaN   \n",
       "1    -3.280218    -2.261787    -3.617442          NaN          NaN   \n",
       "2     1.017174    -1.059342    -1.723941          NaN          NaN   \n",
       "3    -3.416992    -2.645002    -2.973197          NaN          NaN   \n",
       "4     2.417244     0.806850    -1.460248          NaN          NaN   \n",
       "\n",
       "   feature_122  feature_123  feature_124  feature_125  feature_126  \\\n",
       "0     2.433699     4.282284     1.621115     4.331030     2.553220   \n",
       "1     2.053416    -0.493276     1.661974    -1.082122     2.427706   \n",
       "2    -0.702873     4.038753    -0.789767     4.133183    -1.207878   \n",
       "3     2.304354     1.530169     3.596848     4.613493     4.516110   \n",
       "4    -1.364269     2.006926    -1.237922     2.124396    -2.005723   \n",
       "\n",
       "   feature_127  feature_128  feature_129   ts_id  \n",
       "0     3.799011     2.642943     3.998054  527894  \n",
       "1    -0.756115     2.210572    -0.639075  527895  \n",
       "2     3.402796    -0.928290     3.511141  527896  \n",
       "3     3.341374     2.635798     1.535235  527897  \n",
       "4     1.716914    -1.646484     1.795211  527898  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this data is already excluded day <= 85\n",
    "\n",
    "data = pd.read_parquet('../input/janestreet/data.parquet')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select trade with weight !=  0:\n",
    "data = data[data['weight'] != 0]\n",
    "# # limit memory use: we change datatype from float64 to float32\n",
    "data = data.astype({c: np.float32 for c in data.select_dtypes(include='float64').columns}) \n",
    "\n",
    "# create target variable\n",
    "data['action'] = (data['resp'] > 0)*1\n",
    "\n",
    "# fill null values with mean of each feature\n",
    "data.fillna(data.mean(),inplace=True)\n",
    "\n",
    "#create fetures list\n",
    "features = [c for c in data.columns if 'feature' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1571415 entries, 0 to 1862595\n",
      "Columns: 139 entries, date to action\n",
      "dtypes: float32(135), int64(4)\n",
      "memory usage: 869.2 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info()\n",
    "data.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 / 80 split\n",
    "df_trainvalid, df_test = np.split(data, [int(.8*len(data))])\n",
    "\n",
    "df_train, df_valid = np.split(df_trainvalid, [int(.9*len(df_trainvalid))])\n",
    "\n",
    "X_train = df_train[features]\n",
    "y_train = df_train['action']\n",
    "\n",
    "X_valid = df_valid[features]\n",
    "y_valid = df_valid['action']\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test['action']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_NONE = 0\n",
    "NORMALIZE_MIN_MAX = 1\n",
    "NORMALIZE_MEAN = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    if NORMALIZE_TYPE == NORMALIZE_MIN_MAX:\n",
    "        return (df-df.min())/(df.max()-df.min())\n",
    "    elif NORMALIZE_TYPE == NORMALIZE_MEAN:\n",
    "        return (df-df.mean())/df.std()\n",
    "    else:\n",
    "        return df;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_TYPE = NORMALIZE_MEAN\n",
    "\n",
    "X_train = normalize_data(X_train).values\n",
    "X_valid = normalize_data(X_valid).values\n",
    "X_test = normalize_data(X_test).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50).fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_valid = pca.transform(X_valid)\n",
    "X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data, df_train, df_valid, df_trainvalid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "step_train = len(X_train)//batch_size\n",
    "step_valid = len(X_valid)//batch_size\n",
    "\n",
    "\n",
    "# Create train, valid, test data with batch size \n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).repeat()\n",
    "val = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(batch_size).repeat()\n",
    "test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).repeat()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, y_train, X_valid, y_valid, y_test\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               6528      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 41,217\n",
      "Trainable params: 40,449\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l_r = 1e-4\n",
    "\n",
    "# A Sequential model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(50,)),\n",
    "    \n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(1),\n",
    "    tf.keras.layers.Activation(\"sigmoid\")\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate = l_r),metrics=[\"AUC\"])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "276/276 [==============================] - 6s 14ms/step - loss: 0.7528 - auc: 0.5025 - val_loss: 0.6981 - val_auc: 0.5095\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.7230 - auc: 0.5060 - val_loss: 0.6965 - val_auc: 0.5101\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.7148 - auc: 0.5066 - val_loss: 0.6956 - val_auc: 0.5098\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 3s 12ms/step - loss: 0.7092 - auc: 0.5085 - val_loss: 0.6949 - val_auc: 0.5100\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 3s 12ms/step - loss: 0.7054 - auc: 0.5092 - val_loss: 0.6947 - val_auc: 0.5096\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.7027 - auc: 0.5094 - val_loss: 0.6944 - val_auc: 0.5102\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.7006 - auc: 0.5107 - val_loss: 0.6943 - val_auc: 0.5100\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 3s 12ms/step - loss: 0.6987 - auc: 0.5120 - val_loss: 0.6940 - val_auc: 0.5106\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6976 - auc: 0.5121 - val_loss: 0.6938 - val_auc: 0.5111\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6966 - auc: 0.5130 - val_loss: 0.6937 - val_auc: 0.5120\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6957 - auc: 0.5144 - val_loss: 0.6936 - val_auc: 0.5127\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 3s 12ms/step - loss: 0.6952 - auc: 0.5140 - val_loss: 0.6935 - val_auc: 0.5135\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6945 - auc: 0.5166 - val_loss: 0.6935 - val_auc: 0.5136\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6940 - auc: 0.5174 - val_loss: 0.6934 - val_auc: 0.5137\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 4s 13ms/step - loss: 0.6934 - auc: 0.5200 - val_loss: 0.6934 - val_auc: 0.5140\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6935 - auc: 0.5191 - val_loss: 0.6934 - val_auc: 0.5139\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6930 - auc: 0.5209 - val_loss: 0.6934 - val_auc: 0.5148\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6929 - auc: 0.5213 - val_loss: 0.6934 - val_auc: 0.5147\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 3s 12ms/step - loss: 0.6928 - auc: 0.5217 - val_loss: 0.6934 - val_auc: 0.5147\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6926 - auc: 0.5231 - val_loss: 0.6935 - val_auc: 0.5151\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6925 - auc: 0.5229 - val_loss: 0.6935 - val_auc: 0.5152\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6923 - auc: 0.5243 - val_loss: 0.6935 - val_auc: 0.5150\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 3s 12ms/step - loss: 0.6921 - auc: 0.5256 - val_loss: 0.6935 - val_auc: 0.5150\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6918 - auc: 0.5277 - val_loss: 0.6936 - val_auc: 0.5149\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6918 - auc: 0.5277 - val_loss: 0.6937 - val_auc: 0.5149\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 4s 13ms/step - loss: 0.6917 - auc: 0.5280 - val_loss: 0.6937 - val_auc: 0.5147\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 0.6916 - auc: 0.5286 - val_loss: 0.6937 - val_auc: 0.5145\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff912a11210>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, validation_data=val, epochs=30, steps_per_epoch = step_train, validation_steps = step_valid, verbose = 1,callbacks = [\n",
    "                EarlyStopping(monitor='loss', verbose=1, patience=10),\n",
    "                EarlyStopping(monitor='val_loss', verbose=1, patience=10)\n",
    "          ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "76/76 [==============================] - 1s 7ms/step - loss: 0.6915 - auc: 0.5304\n",
      "test loss, test auc: [0.6915105581283569, 0.5303674340248108]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(test, steps = len(X_test)//batch_size)\n",
    "\n",
    "print(\"test loss, test auc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_classes(X_test).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Resp</th>\n",
       "      <th>Action</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1493256</th>\n",
       "      <td>430</td>\n",
       "      <td>0.195581</td>\n",
       "      <td>0.032779</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493257</th>\n",
       "      <td>430</td>\n",
       "      <td>11.982266</td>\n",
       "      <td>-0.004582</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493258</th>\n",
       "      <td>430</td>\n",
       "      <td>1.107787</td>\n",
       "      <td>-0.004491</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.004975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493259</th>\n",
       "      <td>430</td>\n",
       "      <td>1.312454</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493260</th>\n",
       "      <td>430</td>\n",
       "      <td>0.422074</td>\n",
       "      <td>-0.004623</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Weight      Resp  Action         P\n",
       "1493256   430   0.195581  0.032779       1  0.006411\n",
       "1493257   430  11.982266 -0.004582       0 -0.000000\n",
       "1493258   430   1.107787 -0.004491       1 -0.004975\n",
       "1493259   430   1.312454  0.000542       1  0.000711\n",
       "1493260   430   0.422074 -0.004623       1 -0.001951"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame({'Date': df_test['date'], 'Weight': df_test['weight'], 'Resp': df_test['resp'], 'Action': prediction[0]} )\n",
    "result_df['P'] = result_df['Weight']*result_df['Resp']*result_df['Action']\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430</td>\n",
       "      <td>-8.400912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>431</td>\n",
       "      <td>-11.454133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>432</td>\n",
       "      <td>-6.186859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>433</td>\n",
       "      <td>-8.364620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>434</td>\n",
       "      <td>4.384512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date          P\n",
       "0   430  -8.400912\n",
       "1   431 -11.454133\n",
       "2   432  -6.186859\n",
       "3   433  -8.364620\n",
       "4   434   4.384512"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_groupby_days = result_df[['Date', 'P']].groupby('Date').sum().reset_index()\n",
    "print(result_groupby_days.shape)\n",
    "result_groupby_days.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility score is: 2040.143\n"
     ]
    }
   ],
   "source": [
    "p = result_groupby_days['P'].values\n",
    "\n",
    "t = (np.sum(p)/(np.sqrt(np.sum(p**2))))*np.sqrt(250/len(p))\n",
    "\n",
    "u = min(max(t, 0), 6) * np.sum(p)\n",
    "\n",
    "print(f\"Utility score is: {u:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df[['Date', 'Action']].to_csv('NN_result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
