{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/janestreet/data.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook's approach is using LSTM for times-series method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ver 1: XGB\n",
    "* Ver 2: PCA + XGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings (\"ignore\")\n",
    "import gc  \n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "SEED = 1111\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this data is already excluded day <= 85\n",
    "\n",
    "data = pd.read_parquet('../input/janestreet/data.parquet')\n",
    "\n",
    "# Select trade with weight !=  0:\n",
    "data = data[data['weight'] != 0]\n",
    "# # limit memory use: we change datatype from float64 to float32\n",
    "data = data.astype({c: np.float32 for c in data.select_dtypes(include='float64').columns}) \n",
    "\n",
    "# create target variable\n",
    "data['action'] = (data['resp'] > 0)*1\n",
    "\n",
    "# fill null values with mean of each feature\n",
    "data.fillna(data.mean(),inplace=True)\n",
    "\n",
    "#create fetures list\n",
    "features = [c for c in data.columns if 'feature' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 / 80 split\n",
    "df_trainvalid, df_test = np.split(data, [int(.8*len(data))])\n",
    "\n",
    "df_train, df_valid = np.split(df_trainvalid, [int(.8*len(df_trainvalid))])\n",
    "\n",
    "X_train = df_train[features]\n",
    "y_train = df_train['action']\n",
    "\n",
    "X_valid = df_valid[features]\n",
    "y_valid = df_valid['action']\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test['action']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_NONE = 0\n",
    "NORMALIZE_MIN_MAX = 1\n",
    "NORMALIZE_MEAN = 2\n",
    "\n",
    "def normalize_data(df):\n",
    "    if NORMALIZE_TYPE == NORMALIZE_MIN_MAX:\n",
    "        return (df-df.min())/(df.max()-df.min())\n",
    "    elif NORMALIZE_TYPE == NORMALIZE_MEAN:\n",
    "        return (df-df.mean())/df.std()\n",
    "    else:\n",
    "        return df;\n",
    "    \n",
    "NORMALIZE_TYPE = NORMALIZE_MEAN\n",
    "\n",
    "X_train = normalize_data(X_train)\n",
    "X_valid = normalize_data(X_valid)\n",
    "X_test = normalize_data(X_test)\n",
    "\n",
    "X_trainvalid = normalize_data(df_trainvalid[features])\n",
    "y_trainvalid = df_trainvalid['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50).fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_valid = pca.transform(X_valid)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "X_trainvalid = pca.transform(X_trainvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the XGboost-specific DMatrix data format from the numpy array. \n",
    "# This data structure is optimised for memory efficiency and training speed\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data, df_train, df_valid, df_trainvalid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective function is passed an Optuna specific argument of trial\n",
    "def objective(trial):\n",
    "    \n",
    "# params specifies the XGBoost hyperparameters to be tuned\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 25),\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n",
    "        'gamma': trial.suggest_int('gamma', 0, 10),\n",
    "        'tree_method': 'gpu_hist',  \n",
    "        'objective': 'binary:logistic'\n",
    "    }\n",
    "    \n",
    "    bst = xgb.train(params, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    pred_labels = np.rint(preds)\n",
    "# trials will be evaluated based on their accuracy on the test set\n",
    "    accuracy = accuracy_score(y_valid, pred_labels)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:35:08,368]\u001b[0m A new study created in memory with name: no-name-3faf5fe9-173d-4d14-a069-4b4d2c1e369b\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:35:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:35:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:35:11,355]\u001b[0m Trial 0 finished with value: 0.5063815739757465 and parameters: {'n_estimators': 211, 'max_depth': 15, 'learning_rate': 0.02452998267512899, 'subsample': 0.504538123835391, 'colsample_bytree': 0.77820350802674, 'gamma': 8}. Best is trial 0 with value: 0.5063815739757465.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:35:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:35:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:35:26,175]\u001b[0m Trial 1 finished with value: 0.504651449526105 and parameters: {'n_estimators': 388, 'max_depth': 18, 'learning_rate': 0.040304793699077225, 'subsample': 0.6395035247129126, 'colsample_bytree': 0.6618539275128126, 'gamma': 1}. Best is trial 0 with value: 0.5063815739757465.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:35:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:35:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:35:27,085]\u001b[0m Trial 2 finished with value: 0.5082628357336324 and parameters: {'n_estimators': 564, 'max_depth': 11, 'learning_rate': 0.07746034282162846, 'subsample': 0.8606702302304305, 'colsample_bytree': 0.9835078200035376, 'gamma': 10}. Best is trial 2 with value: 0.5082628357336324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:35:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:35:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:17,100]\u001b[0m Trial 3 finished with value: 0.5032912137519041 and parameters: {'n_estimators': 397, 'max_depth': 24, 'learning_rate': 0.08492507653468309, 'subsample': 0.9826057901566732, 'colsample_bytree': 0.6047665722335045, 'gamma': 2}. Best is trial 2 with value: 0.5082628357336324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:31,921]\u001b[0m Trial 4 finished with value: 0.5058247523137929 and parameters: {'n_estimators': 527, 'max_depth': 22, 'learning_rate': 0.026656033590940167, 'subsample': 0.9304770497248106, 'colsample_bytree': 0.8135561397335315, 'gamma': 6}. Best is trial 2 with value: 0.5082628357336324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:31] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:35,332]\u001b[0m Trial 5 finished with value: 0.5066281664260401 and parameters: {'n_estimators': 399, 'max_depth': 25, 'learning_rate': 0.06997614779510761, 'subsample': 0.5815197869807773, 'colsample_bytree': 0.7198439806310271, 'gamma': 10}. Best is trial 2 with value: 0.5082628357336324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:35] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:36,528]\u001b[0m Trial 6 finished with value: 0.5080759027471194 and parameters: {'n_estimators': 389, 'max_depth': 11, 'learning_rate': 0.09814022157778256, 'subsample': 0.6555624453592859, 'colsample_bytree': 0.557994168026312, 'gamma': 1}. Best is trial 2 with value: 0.5082628357336324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:36] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:39,206]\u001b[0m Trial 7 finished with value: 0.5052241803784001 and parameters: {'n_estimators': 409, 'max_depth': 16, 'learning_rate': 0.0933440328205724, 'subsample': 0.8346053148297473, 'colsample_bytree': 0.5262333664712917, 'gamma': 7}. Best is trial 2 with value: 0.5082628357336324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:39] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:43,242]\u001b[0m Trial 8 finished with value: 0.5065685069622594 and parameters: {'n_estimators': 203, 'max_depth': 23, 'learning_rate': 0.05856389845659468, 'subsample': 0.7064349840106293, 'colsample_bytree': 0.9735777146954059, 'gamma': 9}. Best is trial 2 with value: 0.5082628357336324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:43] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:51,072]\u001b[0m Trial 9 finished with value: 0.5056736150055483 and parameters: {'n_estimators': 203, 'max_depth': 16, 'learning_rate': 0.05663846783572531, 'subsample': 0.7381475879407631, 'colsample_bytree': 0.7896841920190132, 'gamma': 2}. Best is trial 2 with value: 0.5082628357336324.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:52,302]\u001b[0m Trial 10 finished with value: 0.5087082930631953 and parameters: {'n_estimators': 600, 'max_depth': 11, 'learning_rate': 0.07755534788843921, 'subsample': 0.8595852736665608, 'colsample_bytree': 0.9999778055169534, 'gamma': 4}. Best is trial 10 with value: 0.5087082930631953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:53,113]\u001b[0m Trial 11 finished with value: 0.5071969199807499 and parameters: {'n_estimators': 600, 'max_depth': 10, 'learning_rate': 0.0760900591009763, 'subsample': 0.8517146023618044, 'colsample_bytree': 0.9994834727071495, 'gamma': 5}. Best is trial 10 with value: 0.5087082930631953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:53] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:55,017]\u001b[0m Trial 12 finished with value: 0.5070298734821638 and parameters: {'n_estimators': 596, 'max_depth': 12, 'learning_rate': 0.07368733254080126, 'subsample': 0.8429437626921459, 'colsample_bytree': 0.9131564665821954, 'gamma': 4}. Best is trial 10 with value: 0.5087082930631953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:55] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:36:57,543]\u001b[0m Trial 13 finished with value: 0.5093168195937589 and parameters: {'n_estimators': 513, 'max_depth': 13, 'learning_rate': 0.08361444315773094, 'subsample': 0.9116325741982502, 'colsample_bytree': 0.889605369619971, 'gamma': 4}. Best is trial 13 with value: 0.5093168195937589.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:37:00,195]\u001b[0m Trial 14 finished with value: 0.5085531784573654 and parameters: {'n_estimators': 499, 'max_depth': 13, 'learning_rate': 0.09991103316252992, 'subsample': 0.990476253729074, 'colsample_bytree': 0.8944797294962171, 'gamma': 4}. Best is trial 13 with value: 0.5093168195937589.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:37:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:37:17,410]\u001b[0m Trial 15 finished with value: 0.5035258743094417 and parameters: {'n_estimators': 470, 'max_depth': 19, 'learning_rate': 0.08673688282107343, 'subsample': 0.922362813466417, 'colsample_bytree': 0.8759437637126043, 'gamma': 3}. Best is trial 13 with value: 0.5093168195937589.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:37:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:37:20,647]\u001b[0m Trial 16 finished with value: 0.507157147004896 and parameters: {'n_estimators': 319, 'max_depth': 14, 'learning_rate': 0.04221477876602928, 'subsample': 0.7747397439458433, 'colsample_bytree': 0.9369934008222547, 'gamma': 5}. Best is trial 13 with value: 0.5093168195937589.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:37:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:37:21,547]\u001b[0m Trial 17 finished with value: 0.5095753439368087 and parameters: {'n_estimators': 550, 'max_depth': 10, 'learning_rate': 0.06778869537300215, 'subsample': 0.9211624171838538, 'colsample_bytree': 0.8557756580958944, 'gamma': 0}. Best is trial 17 with value: 0.5095753439368087.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:37:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:37:49,822]\u001b[0m Trial 18 finished with value: 0.504786677644008 and parameters: {'n_estimators': 462, 'max_depth': 20, 'learning_rate': 0.0641372783945584, 'subsample': 0.9296767683544079, 'colsample_bytree': 0.8546274663575877, 'gamma': 0}. Best is trial 17 with value: 0.5095753439368087.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:37:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:37:52,811]\u001b[0m Trial 19 finished with value: 0.5093764790575396 and parameters: {'n_estimators': 539, 'max_depth': 13, 'learning_rate': 0.04899994747685207, 'subsample': 0.7919797276718263, 'colsample_bytree': 0.7282250177198949, 'gamma': 0}. Best is trial 17 with value: 0.5095753439368087.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:37:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:37:53,682]\u001b[0m Trial 20 finished with value: 0.5072168064686768 and parameters: {'n_estimators': 553, 'max_depth': 10, 'learning_rate': 0.04521524696088189, 'subsample': 0.7806658918037004, 'colsample_bytree': 0.7068194430557915, 'gamma': 0}. Best is trial 17 with value: 0.5095753439368087.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:53] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:37:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:37:56,722]\u001b[0m Trial 21 finished with value: 0.5065883934501864 and parameters: {'n_estimators': 520, 'max_depth': 13, 'learning_rate': 0.049462787821596604, 'subsample': 0.9062277364579486, 'colsample_bytree': 0.8375494770132891, 'gamma': 0}. Best is trial 17 with value: 0.5095753439368087.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:37:56] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:37:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:38:01,077]\u001b[0m Trial 22 finished with value: 0.5085929514332191 and parameters: {'n_estimators': 560, 'max_depth': 14, 'learning_rate': 0.062306793832496536, 'subsample': 0.9572012524614159, 'colsample_bytree': 0.7252381269269151, 'gamma': 2}. Best is trial 17 with value: 0.5095753439368087.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:01] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:38:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:38:03,037]\u001b[0m Trial 23 finished with value: 0.5078849924630211 and parameters: {'n_estimators': 455, 'max_depth': 12, 'learning_rate': 0.03348268409324293, 'subsample': 0.7941644209447025, 'colsample_bytree': 0.6766636649761969, 'gamma': 1}. Best is trial 17 with value: 0.5095753439368087.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:03] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:38:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-03-11 12:38:10,576]\u001b[0m Trial 24 finished with value: 0.506210550179575 and parameters: {'n_estimators': 525, 'max_depth': 16, 'learning_rate': 0.05039179316980943, 'subsample': 0.8754518282380466, 'colsample_bytree': 0.7632230179714273, 'gamma': 3}. Best is trial 17 with value: 0.5095753439368087.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  25\n",
      "Best trial:\n",
      "  Value: 0.5095753439368087\n",
      "  Params: \n",
      "    n_estimators: 550\n",
      "    max_depth: 10\n",
      "    learning_rate: 0.06778869537300215\n",
      "    subsample: 0.9211624171838538\n",
      "    colsample_bytree: 0.8557756580958944\n",
      "    gamma: 0\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=25, timeout=600)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8557756580958944, gamma=0,\n",
       "              gpu_id=0, importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.06778869537300215, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=550, n_jobs=2, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=0.9211624171838538, tree_method='gpu_hist',\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = trial.params\n",
    "best_params['tree_method'] = 'gpu_hist' \n",
    "best_params['objective'] = 'binary:logistic'\n",
    "\n",
    "# Fit the XGBoost classifier with optimal hyperparameters\n",
    "optimal_clf = xgb.XGBClassifier(**best_params)\n",
    "                                \n",
    "optimal_clf.fit(X_trainvalid, y_trainvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility score is: 1447.022\n"
     ]
    }
   ],
   "source": [
    "prediction = optimal_clf.predict(X_test)\n",
    "result_df = pd.DataFrame({'Date': df_test['date'], 'Weight': df_test['weight'],\n",
    "                          'Resp': df_test['resp'], 'Action': prediction})\n",
    "\n",
    "result_df['P'] = result_df['Weight']*result_df['Resp']*result_df['Action']\n",
    "result_groupby_days = result_df[['Date', 'P']].groupby('Date').sum().reset_index()\n",
    "\n",
    "p = result_groupby_days['P'].values\n",
    "\n",
    "t = (np.sum(p)/(np.sqrt(np.sum(p**2))))*np.sqrt(250/len(p))\n",
    "\n",
    "u = min(max(t, 0), 6) * np.sum(p)\n",
    "\n",
    "print(f\"Utility score is: {u:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
